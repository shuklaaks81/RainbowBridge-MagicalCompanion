# Local LLM Configuration for Special Kids Assistant
# This file configures local LLM profiles for offline usage

# =============================================================================
# LOCAL LLM PROFILES
# =============================================================================

# Profile 1: Ollama (Recommended for beginners)
# Install: curl -fsSL https://ollama.ai/install.sh | sh
# Models: llama2, codellama, mistral, etc.
OLLAMA_ENABLED=True
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2:7b-chat
OLLAMA_TIMEOUT=120

# Profile 2: LocalAI (Self-hosted OpenAI alternative)
# Install: https://localai.io/basics/getting_started/
LOCALAI_ENABLED=False
LOCALAI_BASE_URL=http://localhost:8080
LOCALAI_MODEL=gpt-3.5-turbo
LOCALAI_API_KEY=not-needed

# Profile 3: Text Generation WebUI (Gradio interface)
# Install: https://github.com/oobabooga/text-generation-webui
TEXTGEN_ENABLED=False
TEXTGEN_BASE_URL=http://localhost:5000
TEXTGEN_MODEL=default

# Profile 4: Hugging Face Transformers (Direct Python)
# Uses local transformers models
HF_ENABLED=False
HF_MODEL=microsoft/DialoGPT-medium
HF_DEVICE=cpu  # or "cuda" if you have GPU

# Profile 5: Custom OpenAI-compatible endpoint
CUSTOM_ENABLED=False
CUSTOM_BASE_URL=http://localhost:8000
CUSTOM_API_KEY=your_local_api_key
CUSTOM_MODEL=local-model

# =============================================================================
# LOCAL LLM SETTINGS
# =============================================================================

# Primary local LLM to use (when LOCAL_MODE=True)
PRIMARY_LOCAL_LLM=ollama

# Fallback behavior when local LLM fails
FALLBACK_TO_OPENAI=True
LOCAL_LLM_TIMEOUT=60

# Performance settings for local LLMs
LOCAL_MAX_TOKENS=150
LOCAL_TEMPERATURE=0.7
LOCAL_TOP_P=0.9
LOCAL_CONTEXT_LENGTH=2048

# Child-safety settings for local models
ENABLE_CONTENT_FILTERING=True
CHILD_SAFE_MODE=True

# =============================================================================
# MODEL RECOMMENDATIONS FOR SPECIAL KIDS ASSISTANT
# =============================================================================

# Recommended Ollama models (in order of preference):
# 1. llama2:7b-chat - Good balance of performance and quality
# 2. mistral:7b-instruct - Fast and efficient
# 3. codellama:7b-instruct - Good for structured responses
# 4. neural-chat:7b - Optimized for conversations

# Hardware requirements:
# - Minimum: 8GB RAM for 7B models
# - Recommended: 16GB RAM for better performance
# - GPU: Optional but significantly faster

# Privacy benefits:
# - All processing happens locally
# - No data sent to external servers
# - Complete control over model behavior
# - Works offline
